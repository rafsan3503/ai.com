<!doctype html><html lang=en><head><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta content="Integrated development environment for prompt engineering and interpretability research" name=description><meta content=summary_large_image name=twitter:card><meta content=PromptIDE name=twitter:title><meta content="Integrated development environment for prompt engineering and interpretability research" name=twitter:description><meta content=https://x.ai/x_promptide.jpg name=twitter:image><meta content=https://x.ai/prompt-ide/ property=og:url><meta content=website property=og:type><meta content=PromptIDE property=og:title><meta content="Integrated development environment for prompt engineering and interpretability research" property=og:description><meta content=https://x.ai/x_promptide.jpg property=og:image><link href="https://fonts.googleapis.com/css2?&family=Nunito:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;0,1000;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900;1,1000&display=swap" rel=stylesheet><link href=https://x.ai/favicon.svg rel=icon type=image/svg+xml><title>PromptIDE</title><link href=https://fonts.googleapis.com rel=preconnect><link crossorigin href=https://fonts.gstatic.com rel=preconnect><link as=image href=https://x.ai/bg_square_large.webp rel=preload><link as=image href=https://x.ai/bg_square_small.webp rel=preload><link href=https://x.ai/styles.css rel=stylesheet><body><div class=stars><div class="animation stars-small"></div><div class="animation stars-large"></div></div><header id=header><div class=background></div><div class=container><div class=content><a href=https://x.ai/><img alt="xAI Logo" src=/inverse_xai_logo_5.svg></a><nav><a href=https://x.ai/> Grok</a><a href=/about/> About</a><a href=/career/> Join us</a><a class=active href=/prompt-ide/> PromptIDE</a></nav></div></div></header><div id=swup><section class="page-teaser prompt-ide"><div class=gradient></div><div class=container><div class=hero><h1 class=animated><span>xAI</span> <span>PromptIDE</span></h1><p class=reveal>Integrated development environment for prompt engineering and interpretability research</div></div></section><section class=page><div class=container><div class=page-content><p><small>November 6, 2023</small><p><strong>The xAI PromptIDE is an integrated development environment for prompt engineering and interpretability research. It accelerates prompt engineering through an SDK that allows implementing complex prompting techniques and rich analytics that visualize the network's outputs. We use it heavily in our continuous development of <a href=https://x.ai/>Grok</a>.</strong><p>We developed the PromptIDE to give transparent access to Grok-1, the model that powers <a href=https://x.ai/>Grok</a>, to engineers and researchers in the community. The IDE is designed to empower users and help them explore the capabilities of our large language models (LLMs) at pace. At the heart of the IDE is a Python code editor that - combined with a new <a href=/ide/docs.html>SDK</a> - allows implementing complex prompting techniques. While executing prompts in the IDE, users see helpful analytics such as the precise tokenization, sampling probabilities, alternative tokens, and aggregated attention masks.<p>The IDE also offers quality of life features. It automatically saves all prompts and has built-in versioning. The analytics generated by running a prompt can be stored permanently allowing users to compare the outputs of different prompting techniques. Finally, users can upload small files such as CSV files and read them using a single Python function from the SDK. When combined with the SDK's concurrency features, even somewhat large files can be processed quickly.<p>We also hope to build a community around the PromptIDE. Any prompt can be shared publicly at the click of a button. Users can decide if they only want to share a single version of the prompt or the entire tree. It's also possible to include any stored analytics when sharing a prompt.<p>The PromptIDE is available to members of our early <a href=https://ide.x.ai>access program</a>. Below, you find a walkthrough of the main features of the IDE.<p>Thank you,<br> the xAI Team<h1 id=code-editor-sdk>Code editor & SDK</h1><p><img alt="Sampling probabilities in the PromptIDE" src=/screenshot_42.png><p>At the heart of the PromptIDE is a code editor and a <a href=/ide/docs.html>Python SDK</a>. The SDK provides a new programming paradigm that allows implementing complex prompting techniques elegantly. All Python functions are executed in an implicit context, which is a sequence of tokens. You can manually add tokens to the context using the <code>prompt()</code> function or you can use our models to generate tokens based on the context using the <code>sample()</code> function. When sampling from the model, you have various configuration options that are passed as argument to the function:<pre class=language-python data-lang=python style=background:#2b303b;color:#c0c5ce><code class=language-python data-lang=python><span style=color:#b48ead>async def </span><span style=color:#8fa1b3>sample</span><span>(
</span><span>    </span><span style=color:#bf616a>self</span><span>,
</span><span>    </span><span style=color:#bf616a>max_len</span><span>: int = </span><span style=color:#d08770>256</span><span>,
</span><span>    </span><span style=color:#bf616a>temperature</span><span>: float = </span><span style=color:#d08770>1.0</span><span>,
</span><span>    </span><span style=color:#bf616a>nucleus_p</span><span>: float = </span><span style=color:#d08770>0.7</span><span>,
</span><span>    </span><span style=color:#bf616a>stop_tokens</span><span>: Optional[list[str]] = </span><span style=color:#d08770>None</span><span>,
</span><span>    </span><span style=color:#bf616a>stop_strings</span><span>: Optional[list[str]] = </span><span style=color:#d08770>None</span><span>,
</span><span>    </span><span style=color:#bf616a>rng_seed</span><span>: Optional[int] = </span><span style=color:#d08770>None</span><span>,
</span><span>    </span><span style=color:#bf616a>add_to_context</span><span>: bool = </span><span style=color:#d08770>True</span><span>,
</span><span>    </span><span style=color:#bf616a>return_attention</span><span>: bool = </span><span style=color:#d08770>False</span><span>,
</span><span>    </span><span style=color:#bf616a>allowed_tokens</span><span>: Optional[Sequence[Union[int, str]]] = </span><span style=color:#d08770>None</span><span>,
</span><span>    </span><span style=color:#bf616a>disallowed_tokens</span><span>: Optional[Sequence[Union[int, str]]] = </span><span style=color:#d08770>None</span><span>,
</span><span>    </span><span style=color:#bf616a>augment_tokens</span><span>: bool = </span><span style=color:#d08770>True</span><span>,
</span><span>) -> SampleResult:
</span><span>    </span><span style=color:#65737e>"""Generates a model response based on the current prompt.
</span><span style=color:#65737e>
</span><span style=color:#65737e>    The current prompt consists of all text that has been added to the prompt either since the
</span><span style=color:#65737e>    beginning of the program or since the last call to `clear_prompt`.
</span><span style=color:#65737e>
</span><span style=color:#65737e>    Args:
</span><span style=color:#65737e>        max_len: Maximum number of tokens to generate.
</span><span style=color:#65737e>        temperature: Temperature of the final softmax operation. The lower the temperature, the
</span><span style=color:#65737e>            lower the variance of the token distribution. In the limit, the distribution collapses
</span><span style=color:#65737e>            onto the single token with the highest probability.
</span><span style=color:#65737e>        nucleus_p: Threshold of the Top-P sampling technique: We rank all tokens by their
</span><span style=color:#65737e>            probability and then only actually sample from the set of tokens that ranks in the
</span><span style=color:#65737e>            Top-P percentile of the distribution.
</span><span style=color:#65737e>        stop_tokens: A list of strings, each of which will be mapped independently to a single
</span><span style=color:#65737e>            token. If a string does not map cleanly to one token, it will be silently ignored.
</span><span style=color:#65737e>            If the network samples one of these tokens, sampling is stopped and the stop token
</span><span style=color:#65737e>            *is not* included in the response.
</span><span style=color:#65737e>        stop_strings: A list of strings. If any of these strings occurs in the network output,
</span><span style=color:#65737e>            sampling is stopped but the string that triggered the stop *will be* included in the
</span><span style=color:#65737e>            response. Note that the response may be longer than the stop string. For example, if
</span><span style=color:#65737e>            the stop string is "Hel" and the network predicts the single-token response "Hello",
</span><span style=color:#65737e>            sampling will be stopped but the response will still read "Hello".
</span><span style=color:#65737e>        rng_seed: See of the random number generator used to sample from the model outputs.
</span><span style=color:#65737e>        add_to_context: If true, the generated tokens will be added to the context.
</span><span style=color:#65737e>        return_attention: If true, returns the attention mask. Note that this can significantly
</span><span style=color:#65737e>            increase the response size for long sequences.
</span><span style=color:#65737e>        allowed_tokens: If set, only these tokens can be sampled. Invalid input tokens are
</span><span style=color:#65737e>            ignored. Only one of `allowed_tokens` and `disallowed_tokens` must be set.
</span><span style=color:#65737e>        disallowed_tokens: If set, these tokens cannot be sampled. Invalid input tokens are
</span><span style=color:#65737e>            ignored. Only one of `allowed_tokens` and `disallowed_tokens` must be set.
</span><span style=color:#65737e>        augment_tokens: If true, strings passed to `stop_tokens`, `allowed_tokens` and
</span><span style=color:#65737e>            `disallowed_tokens` will be augmented to include both the passed token and the
</span><span style=color:#65737e>            version with leading whitespace. This is useful because most words have two
</span><span style=color:#65737e>            corresponding vocabulary entries: one with leading whitespace and one without.
</span><span style=color:#65737e>
</span><span style=color:#65737e>    Returns:
</span><span style=color:#65737e>        The generated text.
</span><span style=color:#65737e>    """
</span></code></pre><p>The code is executed locally using an in-browser Python interpreter that runs in a separate web worker. Multiple web workers can run at the same time, which means you can execute many prompts in parallel.<p><img alt="Sampling probabilities in the PromptIDE" src=/screenshot_42_completion.png><p>Complex prompting techniques can be implemented using multiple contexts within the same program. If a function is annotated with the <code>@prompt_fn</code> decorator, it is executed in its own, fresh context. The function can perform some operations independently of its parent context and pass the results back to the caller using the <code>return</code> statement. This programming paradigm enables recursive and iterative prompts with arbitrarily nested sub-contexts.<h2 id=concurrency>Concurrency</h2><p>The SDK uses Python coroutines that enable processing multiple <code>@prompt_fn</code>-annotated Python functions concurrently. This can significantly speed up the time to completion - especially when working with CSV files.<p><img alt="Sampling probabilities in the PromptIDE" src=/screenshot_concurrency.png><h2 id=user-inputs>User inputs</h2><p>Prompts can be made interactive through the <code>user_input()</code> function, which blocks execution until the user has entered a string into a textbox in the UI. The <code>user_input()</code> function returns the string entered by the user, which cen then, for example, be added to the context via the <code>prompt()</code> function. Using these APIs, a chatbot can be implemented in just four lines of code:<pre class=language-python data-lang=python style=background:#2b303b;color:#c0c5ce><code class=language-python data-lang=python><span style=color:#b48ead>await </span><span style=color:#bf616a>prompt</span><span>(</span><span style=color:#bf616a>PREAMBLE</span><span>)
</span><span style=color:#b48ead>while </span><span>text := </span><span style=color:#b48ead>await </span><span style=color:#bf616a>user_input</span><span>("</span><span style=color:#a3be8c>Write a message</span><span>"):
</span><span>    </span><span style=color:#b48ead>await </span><span style=color:#bf616a>prompt</span><span>(</span><span style=color:#b48ead>f</span><span>"</span><span style=color:#a3be8c><|separator|></span><span style=color:#96b5b4>\n\n</span><span style=color:#a3be8c>Human: </span><span>{text}</span><span style=color:#a3be8c><|separator|></span><span style=color:#96b5b4>\n\n</span><span style=color:#a3be8c>Assistant:</span><span>")
</span><span>    </span><span style=color:#b48ead>await </span><span style=color:#bf616a>sample</span><span>(</span><span style=color:#bf616a>max_len</span><span>=</span><span style=color:#d08770>1024</span><span>, </span><span style=color:#bf616a>stop_tokens</span><span>=["</span><span style=color:#a3be8c><|separator|></span><span>"], </span><span style=color:#bf616a>return_attention</span><span>=</span><span style=color:#d08770>True</span><span>)
</span></code></pre><h2 id=files>Files</h2><p>Developers can upload small files to the PromptIDE (up to 5 MiB per file. At most 50 MiB total) and use their uploaded files in the prompt. The <code>read_file()</code> function returns any uploaded file as a byte array. When combined with the concurrency feature mentioned above, this can be used to implement batch processing prompts to evaluate a prompting technique on a variety of problems. The screenshot below shows a prompt that calculates the MMLU evaluation score.<p><img alt="Sampling probabilities in the PromptIDE" src=/screenshot_mmlu2.png><h1 id=analytics>Analytics</h1><p>While executing a prompt, users see detailed per-token analytics to help them better understand the model's output. The completion window shows the precise tokenization of the context alongside the numeric identifiers of each token. When clicking on a token, users also see the top-K tokens after applying top-P thresholding and the aggregated attention mask at the token.<p><img alt="Sampling probabilities in the PromptIDE" src=/screenshot_42_completion.png><p><img alt="Sampling probabilities in the PromptIDE" src=/screenshot_42_token.png><p>When using the <code>user_input()</code> function, a textbox shows up in the window while the prompt is running that users can enter their response into. The below screenshot shows the result of executing the chatbot code snippet listed above.<p><img alt="Sampling probabilities in the PromptIDE" src=/screenshot_chat_completion.png><p>Finally, the context can also be rendered in markdown to improve legibility when the token visualization features are not required.<p><img alt="Sampling probabilities in the PromptIDE" src=/screenshot_completion_markdown.png></div></div></section></div><footer><div class=container><div class=footer-content><img alt="xAI Logo" src=/inverse_xai_logo_5.svg><div class=left>xAI Corp (<a href=https://x.com/xai>@xAI</a>) Â© 2023<br> All rights reserved.</div><div class=right><a href=/legal/terms-of-service/> Terms of use </a><br><a href=/legal/privacy-policy/> Privacy Policy </a><br></div></div></div></footer><script [data-swup-ignore-script] src=https://x.ai/main.js></script>